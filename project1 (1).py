# -*- coding: utf-8 -*-
"""Project1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ksuQWTXtlRogcHm5DK3buSgPwJUfPXEo
"""

import pandas as pd
df = pd.read_excel('CTG.xls','Data2')
df.head()

df = df.drop(df.columns[df.columns.str.contains('Unnamed', na=False)], axis=1)
df = df.drop(df.columns[0:9], axis=1)

df.head()

df.isnull().sum()

df=df.dropna()
df.isnull().sum()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

correlation_with_target = df.corr()[['NSP']]
print("Correlation with Target Column:")
print(correlation_with_target)

sns.set(style="whitegrid")
plt.figure(figsize=(10, 6))
sns.barplot(x=correlation_with_target.index, y=correlation_with_target['NSP'])
plt.xticks(rotation=90)
plt.title(f'Correlation with NSP')
plt.show()

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import chi2_contingency


categorical_features = ['A','B','C','D','E', 'AD','DE','LD','FS','SUSP','CLASS']
target_variable = 'NSP'

contingency_tables = pd.DataFrame(index=categorical_features, columns=[target_variable])

for feature in categorical_features:
    contingency_table = pd.crosstab(df[feature], df[target_variable])
    chi2, _, _, _ = chi2_contingency(contingency_table)
    n = np.sum(contingency_table.sum())
    cramers_v = np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))
    contingency_tables.at[feature, target_variable] = cramers_v

contingency_tables = contingency_tables.apply(pd.to_numeric)

plt.figure(figsize=(10, 6))
sns.heatmap(contingency_tables, annot=True, cmap="coolwarm", fmt=".2f", linewidths=.5)
plt.title(f'Cram√©r\'s V Heatmap with {target_variable}')
plt.show()

plt.figure(figsize=(16, 8))
cols = [ 'DP.1', 'ASTV','ALTV']
sns.boxplot(data=df[cols], orient="h", palette="Set2")
plt.title('Box Plots for Outliers in Columns')
plt.show()

sns.set(style="whitegrid")
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))
sns.countplot(x='LD', data=df, ax=axes[0], palette='Set2')
sns.countplot(x='FS', data=df, ax=axes[1], palette='Set2')
sns.countplot(x='CLASS', data=df, ax=axes[2], palette='Set2')
plt.suptitle('Count Plots of Categorical Features', y=1.02)
plt.tight_layout()
plt.show()

sns.set(style="whitegrid")
df[['DP.1', 'ASTV', 'ALTV']].hist(bins=30, figsize=(12, 6), edgecolor='black', layout=(1, 3))
plt.suptitle('Histograms of Continuous Features', y=1.02)
plt.show()

print(df['DP.1'].skew())
print(df.ASTV.skew())
print(df.ALTV.skew())

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder
X = df[['LD', 'FS', 'CLASS',
        'DP.1', 'ASTV', 'ALTV']]
y = df['NSP']

X = pd.get_dummies(X, columns=['LD', 'FS', 'CLASS'], drop_first=True)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)
clf_rf.fit(X_train, y_train)
y_pred = clf_rf.predict(X_test)
accuracy_rf = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy_rf:.2f}')
print('\nClassification Report:\n', classification_report(y_test, y_pred))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
cf= confusion_matrix(y_test, y_pred)
plt.figure()
sns.heatmap(cf, annot=True)
plt.xlabel('Prediction')
plt.ylabel('Target')
plt.title('Confusion Matrix for Random Forest')

from xgboost import XGBClassifier
clf_xgb = XGBClassifier(n_estimators=100, random_state=42)
clf_xgb.fit(X_train, y_train)
y_pred = clf_xgb.predict(X_test)
accuracy_xgb = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy_xgb:.2f}')
print('\nClassification Report:\n', classification_report(y_test, y_pred))

cf= confusion_matrix(y_test, y_pred)
plt.figure()
sns.heatmap(cf, annot=True)
plt.xlabel('Prediction')
plt.ylabel('Target')
plt.title('Confusion Matrix for XGB')

from sklearn.svm import SVC
clf_svm = SVC(kernel='linear', C=1.0, random_state=42)
clf_svm.fit(X_train, y_train)
y_pred_svm = clf_svm.predict(X_test)
accuracy_svm = accuracy_score(y_test, y_pred_svm)
print(f'SVM Accuracy: {accuracy_svm:.2f}')
print('\nSVM Classification Report:\n', classification_report(y_test, y_pred_svm))

cf= confusion_matrix(y_test, y_pred)
plt.figure()
sns.heatmap(cf, annot=True)
plt.xlabel('Prediction')
plt.ylabel('Target')
plt.title('Confusion Matrix for SVM')

from sklearn.neighbors import KNeighborsClassifier
clf_knn = KNeighborsClassifier(n_neighbors=5)
clf_knn.fit(X_train, y_train)
y_pred_knn = clf_knn.predict(X_test)
accuracy_knn = accuracy_score(y_test, y_pred_knn)
print(f'KNN Accuracy: {accuracy_knn:.2f}')
print('\nKNN Classification Report:\n', classification_report(y_test, y_pred_knn))

cf= confusion_matrix(y_test, y_pred)
plt.figure()
sns.heatmap(cf, annot=True)
plt.xlabel('Prediction')
plt.ylabel('Target')
plt.title('Confusion Matrix for KNN')

from lightgbm import LGBMClassifier
clf_lgbm = LGBMClassifier(n_estimators=100, random_state=42)
clf_lgbm.fit(X_train, y_train)
y_pred_lgbm = clf_lgbm.predict(X_test)
accuracy_lgbm = accuracy_score(y_test, y_pred_lgbm)
print(f'LGBM Accuracy: {accuracy_lgbm:.2f}')
print('\nLGBM Classification Report:\n', classification_report(y_test, y_pred_lgbm))

cf= confusion_matrix(y_test, y_pred)
plt.figure()
sns.heatmap(cf, annot=True)
plt.xlabel('Prediction')
plt.ylabel('Target')
plt.title('Confusion Matrix for LGBM')

from sklearn.ensemble import VotingClassifier
svm_clf = SVC(kernel='linear', C=1.0, probability=True, random_state=42)
xgb_clf = XGBClassifier(n_estimators=100, random_state=42)
lgbm_clf = LGBMClassifier(n_estimators=100, random_state=42)
knn_clf = KNeighborsClassifier(n_neighbors=5)
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)
ensemble_clf = VotingClassifier(estimators=[
    ('svm', svm_clf),
    ('xgb', xgb_clf),
    ('lgbm', lgbm_clf),
    ('knn', knn_clf),
    ('rf', rf_clf)
], voting='soft')
ensemble_clf.fit(X_train, y_train)
y_pred_ensemble = ensemble_clf.predict(X_test)
accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)
print(f'Ensemble Model Accuracy: {accuracy_ensemble:.2f}')
print('\nEnsemble Model Classification Report:\n', classification_report(y_test, y_pred_ensemble))

cf= confusion_matrix(y_test, y_pred)
plt.figure()
sns.heatmap(cf, annot=True)
plt.xlabel('Prediction')
plt.ylabel('Target')
plt.title('Confusion Matrix for Ensemble Model')

accuracies = [
    ('Random Forest', accuracy_rf),
    ('XGBoost', accuracy_xgb),
    ('SVM', accuracy_svm),
    ('KNN', accuracy_knn),
    ('LGBM', accuracy_lgbm),
    ('Ensemble', accuracy_ensemble)
]

accuracies_df = pd.DataFrame(accuracies, columns=['Classifier', 'Accuracy'])
plt.figure(figsize=(10, 6))
sns.set(style="whitegrid")
sns.barplot(x='Classifier', y='Accuracy', data=accuracies_df, palette="viridis")
plt.title('Classifier Accuracies Comparison')
plt.yticks([i/10 for i in range(11)])  # Set y-axis ticks in intervals of 0.1
plt.ylim(0, 1)
plt.show()